{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提升树"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "思路：对于一个复杂的任务来说，将多个专家的判断结果进行恰当的综合所得出的判断，要比其中任何一个专家的好。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "对提升方法来说，有两个问题需要回答：\n",
    "   1.在每一轮如何改变训练数据的权值或概率分布；\n",
    "   2. 如何将弱分类器组合成一个强分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "问题1: \n",
    "    提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值\n",
    "问题2:\n",
    "    加权多数表决的方法：加大分类误差率小的弱分类器的权值，使其在表决中起较大作用，减少分类误差大的弱分类器的权值，使其在表决中起较小的作用。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 算法"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "输入： 训练集数据 T= {(x1,y1),.....(xn,yn},\n",
    "输出：最终分类器G(X)\n",
    "    \n",
    "    1) 初始化训练数据的权值分布，D_1 = (w_11,w_12,.....w_1n), w1.=1/n\n",
    "    2) 对 m = 1,2,3,....M\n",
    "           a.使具有权值分布Dm 的训练数据集学习，得到基分类器：G_m(X)\n",
    "           b.计算Gm(X)在训练数据集上的分类误差率： e_m = P(G_m(xi)!=yi)\n",
    "           c.计算Gm(X)的系数，a_m = 1/2log((1-e_m)/e_m)\n",
    "           d.更新训练数据集的权值分布：\n",
    "               D_m+1 =(w_(m+1)1, w_(m+1)2 ....w_(m+1)n)\n",
    "               w_(m+1)i = w_mi/Z_m* exp(-a_m*yi*G_m(xi))\n",
    "               Z_m = sum(w_mi*exp(-a_m*yi*G_m(xi))\n",
    "               注： 误分类样本的权值被放大 e_m/(1-e_m)倍\n",
    "   3) 构建基本分类器的线性组合\n",
    "         f(x) = sum(a_m*G_m(x)\n",
    "         G(X_ = sign(f(x)) = sign(sum(a_m*G_m(x)))\n",
    "         "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " AdaBoost算法是：\n",
    "     模型为加法模型，\n",
    "     损失函数为指数损失\n",
    "     学习算法为前向分布算法\n",
    "     的二分类学习方法"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "前向分布算法：\n",
    "\n",
    "输入：训练集数据 T= {(x1,y1),.....(xn,yn}；损失函数L(y,f(x)),基函数集{b(x;r}\n",
    "输出：加法模型\n",
    "    \n",
    "    1) 初始化f0(X) = 0\n",
    "    2) 对 m = 1,2,3,....M\n",
    "           a.极小化损失函数： （p_m,r_m) = argmin(p,r)sum(yi,f(m-1)(xi)+p*b(xi,r)),得到参数p_m, r_m\n",
    "           b.更新： fm(x) = f(m-1)(x) + p_m*b(x,r_m)\n",
    "           c.得到加法模型： f(x) = f(M)(x) = sum(p_m*b(x,r_m)\n",
    "           d.更新训练数据集的权值分布：\n",
    "               D_m+1 =(w_(m+1)1, w_(m+1)2 ....w_(m+1)n)\n",
    "               w_(m+1)i = w_mi/Z_m* exp(-a_m*yi*G_m(xi))\n",
    "               Z_m = sum(w_mi*exp(-a_m*yi*G_m(xi))\n",
    "               注： 误分类样本的权值被放大 e_m/(1-e_m)倍\n",
    "   3) 构建基本分类器的线性组合\n",
    "         f(x) = sum(a_m*G_m(x)\n",
    "         G(X_ = sign(f(x)) = sign(sum(a_m*G_m(x)))\n",
    "         "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "提升树算法：\n",
    "   采用加法模型与前向分布算法，以决策树为基函数的提升方法称为提升树。\n",
    "     f_M(x) = sum(T(x,Q_m)\n",
    "      \n",
    "     1) 确定初始提升树  f_0(x) = 0\n",
    "     2) 第m步的模型为： f_m(x) = f_(m-1)(x) + T(x;Q_m)\n",
    "            通过经验风险极小化确定下一颗决策树的参数Q_m:\n",
    "            hat(Q_m) = argmin(sum(L(yi,f_(m-1)(xi) + T(xi;Q_m)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  gbdt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "对提升方法来说，有两个问题需要回答：\n",
    "   1.在每一轮迭代拟合样本真实值与当前分类器的残差：\n",
    "     Fm(x) = Fm-1(x)+r_m*h_m(x)\n",
    "     h_m(x) = argmin(sum(L(yi,F_[m-1](xi)+h(xi)))\n",
    "     每一轮建树时，样本从原始训练集中采用无放回随机抽样的方式产生。\n",
    "   2. 如何将弱分类器组合成一个强分类器。\n",
    "     控制学习率a:对于同样的训练集学习效果，较小的a意味着需要更多的弱学习器的迭代次数。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "输入：训练数据集T={(x1,y1),(x2,y2)...(xn,yx)}, 损失函数L(y,f(x))；\n",
    "输出： 回归树 f(x)\n",
    "\n",
    "  1) 初始化：\n",
    "     f0 = argmin(sum(L(y,c))) \n",
    "     估计使得损失函数最小化的常数值，它是只有一个根结点的树\n",
    "  2) 对 m = 1,2,...M\n",
    "     a.对 i = 1,2,...N ,计算\n",
    "        r_mi = -[p(L(yi,f(xi))]/p(f(xi) |f(x) = f_{m-1}(x)\n",
    "        \n",
    "        计算损失函数的负梯度在当前模型的值，将它作为残差的估计，对于平方损失函数，它就是通常所说的残差；\n",
    "        对于一般损失函数，它就是估计残差的近似值。\n",
    "     b.对 r_mi 拟合一个回归树，得到第 m 棵树的叶子结点区域 R_{mj} j = 1,2,3...J\n",
    "        估计回归树的叶结点区域，以拟合残差的近似值\n",
    "     c. 对 j = 1,2,...J 计算\n",
    "         c_{mj} = argmin(sum(L(yi,f_{m-1}(xi)+c))\n",
    "         利用线性搜索估计叶结点区域的值，使得损失函数最小化。\n",
    "     d.更新 \n",
    "        f_{m}(x) = f_{m-1}(x)+ sum(c_{mj}I(x<R_{mj})\n",
    "        更新回归树\n",
    "  3) 得到回归树\n",
    "        f(x) = f_{M}(x) = sum(sum(c_{mj}I(x<R_{mj}))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "正则化的方式：\n",
    "   1）学习率\n",
    "   2）子采样比例\n",
    "   3）弱学习器的正则化剪枝\n",
    "   \n",
    "https://zhuanlan.zhihu.com/p/58105824"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
